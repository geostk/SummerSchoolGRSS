#+TITLE: Feature Extraction
#+SUBTITLE: GRSS Summer School
#+AUTHOR: Mathieu Fauvel
#+EMAIL: mathieu.fauvel@ensat.fr
#+DATE: [2017-04-26 Wed 10:30-12:00]

#+INCLUDE_TAGS: export
#+EXCLUDE_TAGS: noexport
#+LANGUAGE: en
#+OPTIONS: H:3 toc:t tags:nil properties:nil

#+COLUMNS: %40ITEM(Task) %17Effort(Estimated Effort){:} %CLOCKSUM

#+LaTeX_CLASS_OPTIONS: [10pt,aspectratio=1610]

#+BEAMER_THEME: DarkConsole
#+BEAMER_HEADER: \institute{UMR Dynafor}
#+BEAMER_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Outline}\tableofcontents[currentsection]\end{frame}}
#+BEAMER_HEADER: \AtBeginSubsection[]{\begin{frame}<beamer>\frametitle{Outline}\tableofcontents[currentsubsection]\end{frame}}
#+BEAMER_HEADER: \setbeamercovered{again covered={\opaqueness<1->{25}}}
#+BEAMER_HEADER: \usefonttheme[onlymath]{serif}

#+LATEX_HEADER: \usepackage[english]{babel}\usepackage{etex}\usepackage{minted}\usemintedstyle{emacs}
#+LATEX_HEADER: \usepackage{tikz}\usepackage{amsmath}\usepackage[T1]{fontenc}\usepackage{lmodern}%\usepackage{arev}
#+LATEX_HEADER: \usepackage{booktabs}\usepackage[citestyle=alphabetic,bibstyle=authortitle]{biblatex}
#+LATEX_HEADER: \usepackage{pgfplots,pgfplotstable}\usetikzlibrary{pgfplots.groupplots}\usepackage[babel=true,kerning=true]{microtype}\usepackage{smartdiagram}
#+LATEX_HEADER: \addbibresource{fe.bib}
#+LATEX_HEADER: \usetikzlibrary{mindmap,trees,shapes,arrows,spy,3d,decorations.pathmorphing,pgfplots.statistics,pgfplots.dateplot}
#+LATEX_HEADER: \pgfplotsset{compat=newest}

#+LATEX_HEADER: \hypersetup{colorlinks,linkcolor=,urlcolor=magenta}
* Motivations                                                        :export:
*** Illustration
- <1-> *Curse of dimensionality*: it is not possible to get enough data to cover all the observation space.
  #+BEGIN_CENTER
  /High dimensional saces are mostly empty !/
  #+END_CENTER
- <2> Multivariate data live in a lower dimensional space, but which one ?
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tabular}{cc}
    \begin{tikzpicture}
      \begin{axis}[grid=major,small]
        \addplot3 [mesh, samples=15, domain=-5:5] {x+y+1};
      \end{axis}
    \end{tikzpicture}&\begin{tikzpicture}
                       \begin{axis}[grid=major,small]
                         \addplot3 [mesh, samples=15, domain=-5:5] {x*x-2*y+1};
                       \end{axis}
                       \end{tikzpicture}     
  \end{tabular}
  \end{center}

  #+END_EXPORT
*** Application
- Feature extraction is important in remote sensing because:
  + It reduces the size of the data,
  + It limits the spatial and the spectral redundancy,
  + It permits visualization of the data,
  + It mitigates the /curse of dimensionality/.
- Extraction techniques:
  + Spectral
    * Physically based method,
    * Statistical methods.
  + Spatial:
    * Linear filters,
    * Non linear techniques (Mathematical Morphology)

* Physical Indices                                                   :export:
** Introduction
*** Spectral indices
- Spectral indices are a linear/non-linear combination of two (or more) spectral bands.
- They provides information as a /single number/ about:
  + Plant structure,
  + Biochemistry,
  + Humidity,
  + Stress.
- Four main types cite:hrsv:2011:
  #+ATTR_LATEX: :centering :booktabs t
  | Name                                   | Formulae                                                                            |
  |----------------------------------------+-------------------------------------------------------------------------------------|
  | Difference vegetation index            | $R_{\lambda_1} - R_{\lambda_2}$                                                   |
  | Ratio vegetation index                 | $\dfrac{R_{\lambda_1}}{R_{\lambda_2}}$                                            |
  | Normalized difference vegetation index | $\dfrac{R_{\lambda_1} - R_{\lambda_2}}{R_{\lambda_1} + R_{\lambda_2}}$          |
  | Soil-adjusted vegetation index         | $(1+L)\times\dfrac{R_{\lambda_1} - R_{\lambda_2}}{R_{\lambda_1} - R_{\lambda_2}+L}$ |
- /The three last indexes are invariant to  a multiplicative factor/

*** Conventional Indices
Index database : [[http://www.indexdatabase.de/]]

#+ATTR_LATEX: :centering :booktabs t :font \small
| Name                                        | Formulae  ($\lambda$ nm)                                                                                                                        |
|---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| Normalized Difference Vegetation index      | $\dfrac{R_{\lambda_{800}} - R_{\lambda_{670}}}{R_{\lambda_{800}} + R_{\lambda_{670}}}$                                                          |
| Modified Soil-Adjusted Vegetation Index     | $\dfrac{1}{2}\left[2R_{\lambda_{800}}+1 - \sqrt{(2R_{\lambda_{800}}+1)^2-8(R_{\lambda_{800}}-R_{\lambda_{670}})}\right]$                        |
| Modified Chlorophyll Absorption Ratio Index | $\left[(R_{\lambda_{700}}-R_{\lambda_{670}})-0.2(R_{\lambda_{700}}-R_{\lambda_{550}})\right]\times\dfrac{R_{\lambda_{700}}}{R_{\lambda_{670}}}$ |
|---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| Normalized Difference Water Index           | $\dfrac{R_{\lambda_{858}} - R_{\lambda_{1240}}}{R_{\lambda_{858}} + R_{\lambda_{1240}}}$                                                        |
| Datt Reflectance Index                      | $\dfrac{R_{\lambda_{816}} - R_{\lambda_{2218}}}{R_{\lambda_{816}} + R_{\lambda_{2218}}}$                                                        |
|---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| Normalized Difference Redness Index         | $\dfrac{R_{\lambda_{540}} - R_{\lambda_{700}}}{R_{\lambda_{540}} + R_{\lambda_{700}}}$                                                          |
| Soil Brightness Index                       | $0.406R_{\lambda{550}}+0.600R_{\lambda{650}}+0.645R_{\lambda{750}}+0.243R_{\lambda{950}}$                                                       |

** Vegetation Indices
*** Normalized difference vegetation index
#+BEGIN_EXPORT latex
$$\text{NDVI}=\dfrac{R_{\lambda_{800}} - R_{\lambda_{670}}}{R_{\lambda_{800}} + R_{\lambda_{670}}}$$
#+END_EXPORT
- $-1\leq \text{NVDI} \leq 1$
- $\text{NDVI}< 0$: surfaces other that plant cover
- $\text{NDVI}\approx 0$: bare soil
- $\text{NDVI}\geq 0.1$: vegetation cover (higher values correspond to more dense covers)

#+BEGIN_EXPORT latex
\begin{center}
\begin{tikzpicture}
\begin{axis}[xmin=0.4,xmax=1,ymin=0,ymax=1,grid,xlabel=$\lambda~({\mu}m)$,ylabel=Reflectance,width=0.6\linewidth,height=0.3\linewidth,cycle list name=color list]
  \addplot+[mark=none,thick,smooth] file {../Introduction/figures/oak.txt};
  \pgfplotstableread{../Introduction/figures/grass.txt}\loadedtable
  \addplot+[mark=none,smooth,thick] table[x=wave,y=grass] from \loadedtable;
  \addplot+[mark=none,smooth,thick] table[x=wave,y=drygrass] from \loadedtable;
  \pgfplotstableread{../Introduction/figures/talc.txt}\loadtable
  \addplot+[mark=none,smooth,thick] table[x=wave,y=talc] from \loadtable;
  \legend{0.81,0.90, 0.05, -0.03}
\end{axis}
\end{tikzpicture}
\end{center}
#+END_EXPORT
** Case study
*** University of Pavia
**** Images                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[file:./figures/university_color.png]]

**** Parameters                                                    :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
- Peri-urban area
- Rosis-3 sensor
- 103 Spectral bands (400nm-900nm)
- 1.5 meter per pixel spatial resolution
- 610 $\times$ 340 pixels

*** Orfeo-Toolbox
- [[https://www.orfeo-toolbox.org/][OTB]] is a C++ library for remote sensing images processing.
- It is free, open-source and available for most OS (window, apple, linux)
- [[https://www.orfeo-toolbox.org/CookBook/OTB-Applications.html][OTB-Applications]] are set of tools appropriated for big/large images
- They are avalaible from QGIS, Python and Bash
- To compute the NDVI

#+BEGIN_SRC bash :tangle ../Codes/spectral_indices.sh
# Computation of the NDVI
otbcli_BandMath -il ../Data/university.tif -out ../Data/university_ndvi.tif \
		-exp "(im1b83-im1b56)/(im1b83+im1b56)"

# Computation of the SBI
otbcli_BandMath -il ../Data/university.tif -out ../Data/university_sbi.tif \
		-exp "0.406*im1b31 + 0.6*im1b52 + 0.645*im1b73"
#+END_SRC

*** University of Pavia - Spectral Indices
**** Images                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width \linewidth
[[file:./figures/university_color.png]]

**** NDVI                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width \linewidth
[[file:./figures/university_ndvi.png]]

**** SBI                                                           :BMCOL: 
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width \linewidth
[[file:./figures/university_sbi.png]]

*** Where is the vegetation 1/2 ?

  #+BEGIN_EXPORT latex
    \begin{center}
    \begin{tikzpicture}
      \begin{axis}[grid=both,width=0.95\linewidth,height=0.45\linewidth,/pgf/number format/1000 sep={},/pgf/number format/fixed,title=Density plot of the NDVI,xmin=-0.6,xmax=1,ymin=0,ymax=0.01]
        \addplot+[mark=none,thick,smooth] table[x=x,y=y,col sep=comma] {figures/pdf.csv};
        \only<2->{\addplot[red,thick] coordinates {(0.19,0) (0.19,0.008)};
        \addplot[red,thick] coordinates {(0.62,0) (0.62,0.008)}; }     
      \end{axis}
  \end{tikzpicture}
  \end{center}
  #+END_EXPORT

#+BEGIN_SRC bash :tangle ../Codes/spectral_indices.sh
# Segmentation of the NDVI in three classes
otbcli_BandMath -il ../Data/university_ndvi.tif -out ../Data/university_ndvi_segmented.tif \
		-exp "(im1b1<0.19?1:(im1b1<0.62?2:3))"
#+END_SRC

*** Where is the vegetation 2/2 ?
**** Images                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[file:./figures/university_color.png]]

**** NDVI                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[file:./figures/university_ndvi_segmented.png]]

** Question
*** Could you find the good one ?
#+BEGIN_EXPORT latex
 \centerline{\begin{tabular}{cc}
    \includegraphics[width=0.4\linewidth]{figures/image1.jpg} & \begin{tikzpicture}\pgfplotsset{every axis legend/.append style={at={(0.5,1.03)},anchor=south}}
      \begin{axis}[ytick=\empty,xmin=-0.5,xmax=0.9,ymin=0,width=0.5\linewidth,axis y line=center,axis x line=bottom,legend columns=4]
        \pgfplotstableread{figures/ndvi1.txt}\loadedtable
        \addplot[smooth,very thick,dashed,blue] table[x=wave,y=ndvi] from \loadedtable;
        \pgfplotstableread{figures/ndvi2.txt}\loadedtable
        \addplot[smooth,very thick,magenta] table[x=wave,y=ndvi] from \loadedtable;
        \pgfplotstableread{figures/ndvi3.txt}\loadedtable
        \addplot[smooth,very thick,dotted,orange] table[x=wave,y=ndvi] from \loadedtable;
        \pgfplotstableread{figures/ndvi4.txt}\loadedtable
        \addplot[smooth,very thick,dashdotted,green] table[x=wave,y=ndvi] from \loadedtable;
        \legend{ndvi$_1$,ndvi$_2$,ndvi$_3$,ndvi$_4$};
      \end{axis}
    \end{tikzpicture}\\
    Image & NDVI Histogram
\end{tabular}}
#+END_EXPORT
#+LaTeX: \vspace{1cm}

From the histogram, which one does correspond to the NDVI of the image ?
* Statistical Feature Extraction                                     :export:
** Unsupervised
*** Principal Components Analysis
- Linear transformation used to reduce the dimensionality of the data cite:jolliffe2002principal.
  $$ z_i = \langle\mathbf{v}_i,\mathbf{x}\rangle$$
- Find features $\mathbf{z}$ that  account for most of the variability of the data:
  + $z_1,~z_2,~z_3,\ldots$ are mutually uncorrelated,
  + $\text{var}(z_i)$ is as large as possible,
  + $\text{var}(z_1)>\text{var}(z_2)>\text{var}(z_3)>\ldots$

#+BEGIN_EXPORT latex
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[grid,small,width=0.4\linewidth,height=0.32\linewidth,xmin=0,xmax=2.5,ymin=0,ymax=2]
      \addplot[only marks,blue] table[x index=0,y index = 1,col sep =comma] {figures/pca_data.csv};
      \begin{scope}
      \addplot[very thick,red] coordinates { (0.080264,0.83834891)  (2.06023219,1.12070676)};
    \end{scope}
   \end{axis}                                  
  \end{tikzpicture}
\end{center}
#+END_EXPORT
*** Maximization of the variance 1/2
- <1-> Search $\mathbf{v}_1$ such as $\max\text{var}(z_1)$:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \text{var}(z_1) & = & \text{var}(\langle\mathbf{v}_1,\mathbf{x}\rangle)\\
    &=& \mathbf{v}_1^\top\boldsymbol{\Sigma}\mathbf{v}_1
  \end{eqnarray*}
  #+END_EXPORT
- <2-> Indetermined: if $\hat{\mathbf{v}}_1$ maximizes the variance, $\alpha\hat{\mathbf{v}}_1$ too!  Add a constraint: $\langle\mathbf{v}_1,\mathbf{v}_1\rangle=1$
- <3-> Lagrangian:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathcal{L}(\mathbf{v}_1,\lambda_1) = \mathbf{v}_1^\top\boldsymbol{\Sigma}\mathbf{v}_1 + \lambda_1(1- \mathbf{v}_1^\top\mathbf{v}_1)  
  \end{eqnarray*}
  #+END_EXPORT
- <4-> Compute the derivative w.r.t $\mathbf{v}_1$:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
  \frac{\partial\mathcal{L}}{\partial\mathbf{v}_1} = 2\boldsymbol{\Sigma}\mathbf{v}_1-2\lambda_1\mathbf{v}_1
  \end{eqnarray*}
  #+END_EXPORT
- <5-> $\mathbf{v}_1$ is an eigenvector of the covariance matrix of $\mathbf{x}$:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \boldsymbol{\Sigma}\mathbf{v}_1 =\lambda_1\mathbf{v}_1
  \end{eqnarray*}
  #+END_EXPORT
- <6->  $\mathbf{v}_1$ is the eigenvector corresponding to the largest eigenvalues !
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \text{var}(z_1)  =  \mathbf{v}_1^\top\boldsymbol{\Sigma}\mathbf{v}_1 = \lambda_1 \mathbf{v}_1^\top\mathbf{v}_1 = \lambda_1
  \end{eqnarray*}
  #+END_EXPORT
*** Maximization of the variance 2/2
- <1-> Search $\mathbf{v}_2$ such as $\max\text{var}(z_2)$ and $\langle\mathbf{v}_2,\mathbf{v}_2\rangle=1$ and $\langle\mathbf{v}_1,\mathbf{v}_2\rangle=0$
- <2-> Lagrangian:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathcal{L}(\mathbf{v}_2,\lambda_2,\beta_1) = \mathbf{v}_2^\top\boldsymbol{\Sigma}\mathbf{v}_2 + \lambda_2(1- \mathbf{v}_2^\top\mathbf{v}_2) + \beta_1(0 - \mathbf{v}_2^\top\mathbf{v}_1)
  \end{eqnarray*}
  #+END_EXPORT
- <3-> Compute the derivative w.r.t $\mathbf{v}_2$:
   #+BEGIN_EXPORT latex
  \begin{eqnarray*}
  \frac{\partial\mathcal{L}}{\partial\mathbf{v}_2} &=& 2\boldsymbol{\Sigma}\mathbf{v}_2-2\lambda_2\mathbf{v}_2-\beta_1\mathbf{v}_1\\
  \boldsymbol{\Sigma}\mathbf{v}_2 &=& \lambda_2\mathbf{v}_2+2\beta_1\mathbf{v}_1
  \end{eqnarray*}
  #+END_EXPORT
- <4-> At optimality, $\langle\mathbf{v}_1,\mathbf{v}_2\rangle=0$. Left-multiplying by $\mathbf{v}_1^\top$ the above equation:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathbf{v}_1^\top\boldsymbol{\Sigma}\mathbf{v}_2 &=& 2\beta_1 \\
    \lambda_1\mathbf{v}_1^\top\mathbf{v}_2 &=& 2\beta_1 \\
    0 &=& 2\beta_1 \\
  \end{eqnarray*}
  #+END_EXPORT
- <5-> Hence, we have 
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \boldsymbol{\Sigma}\mathbf{v}_2 =\lambda_2\mathbf{v}_2
  \end{eqnarray*}
  #+END_EXPORT
- <6-> $\mathbf{v}_2$ is the eigenvector corresponding the /second largest/ eigenvalues
- <7-> $\mathbf{v}_k$ is the eigenvector corresponding the /$k^{\text{th}}$ largest/ eigenvalues
*** PCA in practice
1) Empirical estimation the mean value:
   #+BEGIN_EXPORT latex
   \begin{eqnarray*}
     \boldsymbol{\mu} = \frac{1}{n}\sum_{i=1}^n\mathbf{x}_i
   \end{eqnarray*}
   #+END_EXPORT
2) Empirical estimation the covariance matrix:
   #+BEGIN_EXPORT latex
   \begin{eqnarray*}
     \boldsymbol{\Sigma} = \frac{1}{n-1}\sum_{i=1}^n(\mathbf{x}_i-\boldsymbol{\mu})(\mathbf{x}_i-\boldsymbol{\mu})^\top
   \end{eqnarray*}
   #+END_EXPORT
3) Compute $p$ first eigenvalues/eigenvectors... How to choose $p$ ? Explained variance: 
   #+BEGIN_EXPORT latex
   $$\frac{\sum_{i=1}^p\lambda_i}{\sum_{i=1}^d\lambda_i}$$
   #+END_EXPORT
4) Tips for high dimensional data set: if $n<d$ see cite:manolakis2016hyperspectral page 420
*** PCA case study 1/3
#+BEGIN_SRC python :tangle ../Codes/pcaPavia.py
import rasterTools as rt
import scipy as sp
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data set
im,GeoT,Proj = rt.open_data('../Data/university.tif')
[h,w,b]=im.shape
im.shape=(h*w,b)
wave = sp.loadtxt('../Data/waves.csv',delimiter=',')

# Do PCA
pca = PCA()
pca.fit(im)

# Save Eigenvectors
D = sp.concatenate((wave[:,sp.newaxis],pca.components_[:3,:].T),axis=1)
sp.savetxt('../FeatureExtraction/figures/pca_pcs.csv',D,delimiter=',')
#+END_SRC

#+BEGIN_SRC python :tangle ../Codes/pcaPavia.py :exports none
# Plot explained variance
l = pca.explained_variance_
print l[:5]
print (l.cumsum()/l.sum())[:5]
#+END_SRC
*** PCA case study 2/3
- Explained variance
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,ylabel=\% of variance,xlabel=Number of principal components,axis y line*=left,yticklabel style=red,ylabel style=red, y axis line style=red,ytick style=red]
        \addplot[thick,mark=*,red] coordinates { (1,0.58318064)  (2,0.94418758)  (3,0.98856319)  (4,0.99157161)  (5,0.99366953)};
      \end{axis}
      \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,axis y line*=right,axis x line=none,ylabel=Variance,yticklabel style=blue,ylabel style=blue, y axis line style=blue,ytick style=blue]
        \addplot[thick,mark=*,blue] coordinates { (1,31328687.9)  (2,19393432.0)  (3,2383874.8)  (4,161613.08)  (5,112701.2)};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
- Principal components
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.9\textwidth,height=0.3\textwidth,grid,xmin=400,xmax=900,cycle list name=color list]
        \addplot+[thick] table[col sep=comma,x index=0,y index=1] {figures/pca_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=2] {figures/pca_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=3] {figures/pca_pcs.csv};
        \legend{pc1,pc2,pc3};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
*** PCA case study 3/3
#+BEGIN_SRC python :tangle ../Codes/pcaPavia.py
# Projection of the first PCs
imp = sp.dot(im,pca.components_[:3,:].T)
imp.shape = (h,w,3)

# Save image
rt.write_data('../Data/pca_university.tif',imp,GeoT,Proj)
#+END_SRC
**** PCA 1                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_pc1.png]]

**** PCA 2                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_pc2.png]]

**** PCA 3                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_pc3.png]]

*** Kernel PCA
- PCA is limited to second order information
- To capture higher-order statistics, it is possible to map the data onto another space $\mathcal{H}$
  #+BEGIN_EXPORT latex
    \begin{eqnarray*}
      \begin{array}{rcl}
        \phi:\mathbb{R}^d &\to&\mathcal{H}\\
        \mathbf{x}&\mapsto&\phi(\mathbf{x}).
      \end{array}
    \end{eqnarray*}
  #+END_EXPORT
- In $\mathcal{H}$, conventional PCA can be applied.
- Using the /kernel trick/ it is possible to directly work on the /kernel matrix/ in $\mathbb{R}^d$
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}\label{kpca:matrix}
   \mathbf{K}=\left(
   \begin{array}{cccc}
   k(\mathbf{x}_1,\mathbf{x}_1) & k(\mathbf{x}_1,\mathbf{x}_2) & \ldots & k(\mathbf{x}_1,\mathbf{x}_n)\\
   k(\mathbf{x}_2,\mathbf{x}_1) & k(\mathbf{x}_2,\mathbf{x}_2) & \ldots & k(\mathbf{x}_2,\mathbf{x}_n)\\ 
   \vdots & \vdots & \ddots & \vdots \\
   k(\mathbf{x}_n,\mathbf{x}_1) & k(\mathbf{x}_n,\mathbf{x}_2) & \ldots & k(\mathbf{x}_n,\mathbf{x}_n)\\
   \end{array}\right).
  \end{eqnarray*}
  #+END_EXPORT
- <2> KPCA versus PCA:

  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tabular}{ccc}
    \begin{tikzpicture}
      \begin{axis}[width=0.3\textwidth,height=0.3\textwidth,grid]
        \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma,meta index=2,x index=0,y index=1] {figures/kpca_data.csv};
      \end{axis}
    \end{tikzpicture}&
    \begin{tikzpicture}
      \begin{axis}[width=0.3\textwidth,height=0.3\textwidth,grid]
        \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma,meta index=2,x index=0,y index=1] {figures/kpca_datap.csv};
      \end{axis}
    \end{tikzpicture}&
                       \begin{tikzpicture}
        \begin{axis}[width=0.3\textwidth,height=0.3\textwidth,ylabel=\% of variance,axis y line*=left,yticklabel style=red,ylabel style=red, y axis line style=red,ytick style=red]
          \addplot[thick,mark=*,red] coordinates { (1,0.171950045779)
            (2,0.293633371022)
            (3,0.41194893578)
            (4,0.481444806977)
            (5,0.54956124474)
            (6,0.612183510855)
            (7,0.673659036749)
            (8,0.721296411495)
            (9,0.767653893262)
            (10,0.80191080235)};
        \end{axis}
        \begin{axis}[width=0.3\textwidth,height=0.3\textwidth,axis y line*=right,axis x line=none,ylabel=Variance,yticklabel style=blue,ylabel style=blue, y axis line style=blue,ytick style=blue]
          \addplot[thick,mark=*,blue] coordinates {(1,57.4446834929)
            (2,40.6516908637)
            (3,39.5265970358)
            (4,23.2170239146)
            (5,22.7561859043)
            (6,20.9207054308)
            (7,20.5376050443)
            (8,15.914586718)
            (9,15.4870029584)
            (10,11.4444709279) };
        \end{axis}
      \end{tikzpicture}         
    \end{tabular}

  \end{center}
  #+END_EXPORT

*** Kernel PCA in practice
- Choose the kernel and its parameters
- Compute the kernel matrix $\mathbf{K}$ for all the pixels (or a subset)
- Center the matrix
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
   \mathbf{K}_c=\mathbf{K}-\mathbf{1}_n\mathbf{K}-\mathbf{K}\mathbf{1}_n+\mathbf{1}_n\mathbf{K}\mathbf{1}_n
  \end{eqnarray*}
  #+END_EXPORT
- Solve the eigenproblems
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \lambda\boldsymbol{\alpha}=\mathbf{K}\boldsymbol{\alpha} \text{ subject to } \|\boldsymbol{\alpha}\|_2 = \frac{1}{\lambda}
  \end{eqnarray*}
  #+END_EXPORT
- Project on the $p$ first /kernel principal components/: $\phi^{kpc}(\mathbf{x})=\begin{bmatrix}\phi^{kpc}_1(\mathbf{x})&\hdots&\phi^{kpc}_p(\mathbf{x})\end{bmatrix}^t$
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \phi^{kpc}_j(\mathbf{x})=\sum_{i=1}^n \alpha_{ki} k(\mathbf{x}_i,\mathbf{x})
  \end{eqnarray*}
  #+END_EXPORT

*** KPCA case study 1/3

From cite:fauvel2009kernel.

#+BEGIN_SRC python :tangle ../Codes/kpcaPavia.py
import rasterTools as rt
import scipy as sp
from sklearn.decomposition import KernelPCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Load data set
im,GeoT,Proj = rt.open_data('../Data/university.tif')
[h,w,b]=im.shape
im.shape=(h*w,b)
wave = sp.loadtxt('../Data/waves.csv',delimiter=',')

# Scale data
sc = StandardScaler()
im = sc.fit_transform(im)

# Do KPCA
kpca = KernelPCA(kernel='rbf',gamma=1.0/b,n_jobs=-1)
kpca.fit(im[::50,:]) # Use a subset of the total number of pixels

#+END_SRC
#+BEGIN_SRC python :tangle ../Codes/kpcaPavia.py :exports none
# Plot explained variance
l = kpca.lambdas_
cl = l.cumsum()/l.sum()
for i in range(10):
    print "({0},{1})".format(i+1,l[i])

for i in range(10):
    print "({0},{1})".format(i+1,cl[i])

# Save Eigenvectors
idx = sp.arange(kpca.alphas_[0,:].size)+1
D = sp.concatenate((idx[:,sp.newaxis],kpca.alphas_[:3,:].T),axis=1)
sp.savetxt('../FeatureExtraction/figures/kpca_pcs.csv',D,delimiter=',')
#+END_SRC

*** KPCA case study 2/3
- Explained variance
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,ylabel=\% of variance,xlabel=Number of principal components,axis y line*=left,yticklabel style=red,ylabel style=red, y axis line style=red,ytick style=red]
        \addplot[thick,mark=*,red] coordinates {(1,0.257631571125)
        (2,0.438129567049)
        (3,0.61420975716)
        (4,0.695091757082)
        (5,0.751533118467)
        (6,0.790148033382)
        (7,0.814644462352)
        (8,0.833924207631)
        (9,0.851128186791)
        (10,0.865878267501) };
      \end{axis}
      \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,axis y line*=right,axis x line=none,ylabel=Variance,yticklabel style=blue,ylabel style=blue, y axis line style=blue,ytick style=blue]
        \addplot[thick,mark=*,blue] coordinates {(1,649.766197024)
        (2,455.229519695)
        (3,444.087481204)
        (4,203.990486367)
        (5,142.349357966)
        (6,97.389719371)
        (7,61.7818360658)
        (8,48.6249674864)
        (9,43.3897292304)
        (10,37.2008127987) };
        \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
- Principal kernel components
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.9\textwidth,height=0.3\textwidth,grid,cycle list name=color list,xmin=0,xmax=4148]
        \addplot+[thick] table[col sep=comma,x index=0,y index=1] {figures/kpca_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=2] {figures/kpca_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=3] {figures/kpca_pcs.csv};
        \legend{kpc1,kpc2,kpc3};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
*** KPCA case study 3/3
#+BEGIN_SRC python :tangle ../Codes/kpcaPavia.py
imp = kpca.transform(im)[:,:3]
imp.shape = (h,w,3)

# Save image
rt.write_data('../Data/kpca_university.tif',imp,GeoT,Proj)
#+END_SRC
**** KPCA 1                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_kpc1.png]]

**** KPCA 2                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_kpc2.png]]

**** KPCA 3                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_kpc3.png]]

** Supervised
*** Fisher's Discriminant Analysis
- We observe some $\{\mathbf{x}_i,y_i\}_{i=1}^n$
- Use the label information to find the linear features that highlight differences among classes

  #+BEGIN_EXPORT latex 
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.6\textwidth,grid,small,xmin=-5,xmax=5,ymin=-5,ymax=5]
        \addplot[scatter, only marks,scatter src=explicit]table[col sep=comma,meta index=2,x index =0,y index=1] {figures/lda_data.csv};      
        \addplot[domain=-5:5,very thick] {-x/0.52306077251960925*0.85229538790913895 - 3.65687201/3.04406312};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
- FDA: find $\mathbf{a}$ such as the ratio between the /between projected variance/ and the /sample projected variance/ is maximal cite:manolakis2016hyperspectral Chap. 8.8
*** FDA Algorithm
- Between-class covariance matrix:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathbf{B} = \frac{1}{n}\sum_{c=1}^Cn_c(\boldsymbol{\mu}_c-\boldsymbol{\mu})(\boldsymbol{\mu}_c-\boldsymbol{\mu})^\top
  \end{eqnarray*}
  #+END_EXPORT
- Class covariance matrix
  #+BEGIN_EXPORT latex
   \begin{eqnarray*}
     \boldsymbol{\Sigma}_c = \frac{1}{n_c-1}\sum_{i=1,i \in c}^{n_c}(\mathbf{x}_i-\boldsymbol{\mu}_c)(\mathbf{x}_i-\boldsymbol{\mu}_c)^\top
   \end{eqnarray*}
   #+END_EXPORT
- Within-class covariance matrix
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathbf{W} = \sum_{c=1}^C\boldsymbol{\Sigma}_c
  \end{eqnarray*}
  #+END_EXPORT
- The Fisher discriminant subspace is given by the eigenvectors of $\mathbf{W}^{(-1)}\mathbf{B}$
- Remark: there are at most $C-1$ eigenvectors because $\text{Rank}(\mathbf{B})\leq C-1$.
*** FDA case study 1/3

#+BEGIN_SRC python :tangle ../Codes/ldaPavia.py
import rasterTools as rt
import scipy as sp
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Load data set
X,y=rt.get_samples_from_roi('../Data/university.tif','../Data/university_gt.tif')
wave = sp.loadtxt('../Data/waves.csv',delimiter=',')

# Select the same number of samples
nt = 900
xt,yt=[],[]
for i in sp.unique(y):
    t = sp.where(y==i)[0]
    nc = t.size
    rp =  sp.random.permutation(nc)
    xt.extend(X[t[rp[0:nt]],:])
    yt.extend(y[t[rp[0:nt]]])

xt = sp.asarray(xt)
yt = sp.asarray(yt)

# Do LDA
lda = LinearDiscriminantAnalysis(solver='eigen',shrinkage='auto')
lda.fit(xt,yt.ravel())
#+END_SRC
#+BEGIN_SRC python :tangle ../Codes/ldaPavia.py :exports none
# Plot explained variance
l = lda.explained_variance_ratio_
cl= l.cumsum()

for i in range(y.max()-1):
    print "({0},{1})".format(i+1,l[i])

for i in range(y.max()-1):
    print "({0},{1})".format(i+1,cl[i])

# Projet data
import matplotlib.pyplot as plt
xp=lda.transform(xt)

# Save projection
D = sp.concatenate((xp[::10,:4],yt[::10]),axis=1)
sp.savetxt("../FeatureExtraction/figures/lda_proj.csv",D,delimiter=',')

# Save Eigenvectors
D = sp.concatenate((wave[:,sp.newaxis],lda.coef_[:3,:].T),axis=1)
sp.savetxt('../FeatureExtraction/figures/lda_pcs.csv',D,delimiter=',')
#+END_SRC

#+RESULTS:
*** FDA case study 2/3
- Projection on Fisher components
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tabular}{cc}
   \begin{tikzpicture}
        \begin{axis}[width=0.4\textwidth,height=0.3\textwidth,xticklabels={,,},yticklabels={,,},grid,xlabel=FC 1,ylabel=FC 2]
          \addplot[scatter,only marks,scatter src=explicit,opacity=0.5] table[col sep =comma,meta index=4,x index=0,y index=1] {figures/lda_proj.csv};
        \end{axis}
      \end{tikzpicture}&
                         \begin{tikzpicture}
                           \begin{axis}[width=0.4\textwidth,height=0.3\textwidth,xticklabels={,,},yticklabels={,,},grid,,xlabel=FC 3,ylabel=FC 4]
                             \addplot[scatter,only marks,scatter src=explicit,opacity=0.5] table[col sep =comma,meta index=4,x index=3,y index=2] {figures/lda_proj.csv};
                           \end{axis}
                         \end{tikzpicture}
    \end{tabular}
  \end{center}
  #+END_EXPORT
- Fisher components
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.9\textwidth,height=0.3\textwidth,grid,xmin=400,xmax=900,cycle list name=color list]
        \addplot+[thick] table[col sep=comma,x index=0,y index=1] {figures/lda_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=2] {figures/lda_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=3] {figures/lda_pcs.csv};
        \legend{pc1,pc2,pc3};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
*** FDA case study 3/3
#+BEGIN_SRC python :tangle ../Codes/ldaPavia.py
im,GeoT,Proj = rt.open_data('../Data/university.tif')
[h,w,b]=im.shape
im.shape=(h*w,b)
imp = lda.transform(im)[:,:3]
imp.shape = (h,w,3)
# Save image
rt.write_data('../Data/lda_university.tif',imp,GeoT,Proj)
#+END_SRC
**** LDA 1                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_lda1.png]]

**** LDA 2                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_lda2.png]]

**** LDA 3                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_lda3.png]]

*** Feature selection
- Feature selection: pick few features /from/ the original ones (no combination)
- In general, for feature selection, we need:
  + /Criterion/ to evaluate how perform the model with a given subset
  + /Optimization   procedure/   to  find   the   subset   that  minimizes/maximizes the criterion
- For instance:   
  #+ATTR_LATEX: :booktabs t
  | Criterion            | Optimization      | Ref.                        |
  |----------------------+-------------------+-----------------------------|
  | Entropy              | Genetic algorithm | cite:chein2007hyperspectral |
  | Jeffries Matusita    | Exhaustive Search | cite:4069122                |
  | Classification error | Forward search/GA | cite:lebris:fs,7847352      |
  | $\ell_1$ norm        | Linear-SVM        | cite:tuia2014automatic      |
  
*** Large scale feature selection with GMM
- Fast  forward strategy  based  on  a nonlinear  model  driven by  an
  estimate of the classification error or a measure of separability:
  #+ATTR_LATEX: :booktabs t
  | Criterion                    | Type       | Complexity |
  |------------------------------+------------+------------|
  | Overall accuracy             | Accuracy   | High       |
  | Cohen's kappa                | Accuracy   | High       |
  | F1 mean                      | Accuracy   | High       |
  |------------------------------+------------+------------|
  | Kullback-Leibler divergences | Divergence | Low        |
  | Jeffries-Matusita distance   | Divergence | Low        |
- Use /Gaussian Mixture Models/ (natural extension for multiclass problem)
- Fast update and fast forward search cite:7847352: based on linear algebra of semi-definite positive matrices

*** Algorithm 1/2
    The forward feature selection works as follow:
    1. Starts with an empty pool $F$ of selected features,
    2. Select the  feature $f_1$ that  provides the best value  for the
       selected criterion and add it to $F$.
    3. Select  the  feature $f_2$  such  that  the couple  of  features
       $(f_1,f_2)$ provides  the best value for  the selected criterion
       and add it to $F$.
    4. Select  the feature  $f_3$  such that  the  triplet of  features
       $(f_1,f_2,f_3)$ ...
    5. ...
    6. The algorithm stops either if the increase of the criterion is too
       low or if the maximum number of features is reached.

*** Algorithm 2/2

#+BEGIN_EXPORT latex
\tikzset{noeud/.style={minimum width=1.5cm,minimum height=1cm,text width = 1.25cm,text centered,rounded corners=1pt,draw,rectangle,thick}}
\tikzstyle{arrow}=[->,>=stealth,thick]
\tikzstyle{arrow2}=[dashed,->,>=stealth,thick]
\centerline{\resizebox{0.65\linewidth}{!}{\begin{tikzpicture}
    % Nodes
    \node[noeud] (S) at (0,0) {$\mathcal{S}$};
    \node[noeud] (S1) at (-4,-2) {$\mathcal{S}_1$};
    \node[noeud] (S2) at (-2,-2) {$\mathcal{S}_2$};
    \node[noeud] (S3) at (0,-2) {$\mathcal{S}_3$};
    \node[noeud] (S4) at (2,-2) {$\mathcal{S}_4$};
    \node[noeud,magenta] (S5) at (4,-2) {$\mathcal{S}_5$};
    \node[noeud] (model) at (-3,0) {Model};
    \node[noeud,orange] (update) at (-2,-4) {Update};
    \node[noeud,orange] (predict) at (2,-4) {OA($\lambda_i$)};
    %Arrows
    \draw[arrow] (S.south)|-(0,-1)-|(S1.north);
    \draw[arrow] (-2,-1)-|(S2.north);
    \draw[arrow] (0,-1)-|(S3.north);
    \draw[arrow] (2,-1)-|(S4.north);
    \draw[arrow] (0,-1)-|(S5.north);
    \draw[arrow] (S.west) -- (model.east);
    \draw[arrow2] (model.west) -| (-5,0) |- (-5,-4) -- (update.west);
    \draw[arrow] (S5.south) |- (2,-3) -| (update.north);
    \draw[arrow2] (update.east) -- (predict.west);
    \draw[arrow] (2,-3) -- (predict.north);
    \draw[dotted,->,>=stealth,thick] (predict.south) -- (2,-5);
    \node (plot) at (-0.7,-7.8) {\begin{axis}[x tick label style={rotate=45,anchor=east},grid,xmin=430,xmax=860,xlabel=$\lambda_i$,ylabel=OA,footnotesize]
        \addplot[thick,smooth] table[x=var,y=error,col sep=comma]  {figures/kcv_1.csv};
      \end{axis}};
\end{tikzpicture}}}
#+END_EXPORT
   
*** FFFS case study 1/3
#+BEGIN_SRC python :exports code :tangle ../Codes/fffsPavia.py
import rasterTools as rt
import scipy as sp
import npfs as npfs

# Load data set
X,y=rt.get_samples_from_roi('../Data/university.tif','../Data/university_gt.tif')
wave = sp.loadtxt('../Data/waves.csv',delimiter=',') 

# Select the same number of samples
nt = 900
xt,yt=[],[]
for i in sp.unique(y):
    t = sp.where(y==i)[0]
    nc = t.size
    rp =  sp.random.permutation(nc)
    xt.extend(X[t[rp[0:nt]],:])
    yt.extend(y[t[rp[0:nt]]])

xt = sp.asarray(xt)
yt = sp.asarray(yt)

# Do FFFS
maxVar = 12
model = npfs.GMMFeaturesSelection()
model.learn_gmm(xt,yt)
idx, crit, [] = model.selection('forward',xt, yt,criterion='kappa', varNb=maxVar, nfold=5)
#+END_SRC

#+BEGIN_SRC python :exports none :tangle ../Codes/fffsPavia.py
for i in range(maxVar):
    print "({0},{1})".format(wave[idx[i]],crit[i])

for i in range(maxVar):
    print "({0},{1})".format(i+1,crit[i])

# Save selected feature
D = sp.copy(model.mean[0,idx[:2]][:,sp.newaxis])

for i in xrange(1,9):
    D = sp.concatenate((D,model.mean[i,idx[:2]][:,sp.newaxis]),axis=1)

D = D.T
C = sp.arange(1,10)
D = sp.concatenate((D,C[:,sp.newaxis]),axis=1)
sp.savetxt("../FeatureExtraction/figures/fffsMean.csv",D,delimiter=',')
#+END_SRC

*** FFFS case study 2/3
- Criterion
  #+BEGIN_EXPORT latex
  \begin{center}
      \begin{tikzpicture}
        \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,ylabel=Kappa coefficient,xlabel=Number of spectral features,grid]
          \addplot[thick,mark=*,] coordinates {(1,0.546805555556)
(2,0.736944444444)
(3,0.847361111111)
(4,0.868888888889)
(5,0.883333333333)
(6,0.894583333333)
(7,0.901805555556)
(8,0.906527777778)
(9,0.907916666667)
(10,0.910972222222)
(11,0.911805555556)
(12,0.913888888889)};
        \end{axis}
      \end{tikzpicture}
    \end{center}
  #+END_EXPORT
- Mean projection on best bands


  #+BEGIN_EXPORT latex
  \begin{center}
  \begin{tikzpicture}                           
  \begin{axis}[width=0.4\textwidth,height=0.3\textwidth,xticklabels={,,},yticklabels={,,},grid,xlabel={$\hat{\lambda}_1 = 555$},ylabel={$\hat{\lambda}_2 = 798$}]
  \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma, meta index=2,x index=0,y index=1] {figures/fffsMean.csv};
  \end{axis}
  \end{tikzpicture}
  \end{center}
  #+END_EXPORT

** Questions
*** Number of features
Given         a         set         of         observed         pixels
$\mathcal{S}=\{(\mathbf{x}_i,y_i)\}_{i=1}^n$                       and
$\mathbf{x}\in\mathbb{R}^{d}$. The number of classes $C$ is $3$.  What
is the /maximum/ number of features that can be extracted with

- <1> PCA, when $d=200$ and $n=100$:  =a) 100, b) 200, c) 400 and d) 2=.
- <2> PCA, when $d=200$ and $n=400$: =a) 100, b) 200, c) 400 and d) 2=.
- <3> FDA, when $d=200$ and $n=400$:  =a) 100, b) 200, c) 400 and d) 2=.
- <4> KPCA, when $d=200$ and $n=400$:  =a) 100, b) 200, c) 400 and d) 2=.
- <5> KPCA, when $d=200$ and $n=100$:  =a) 100, b) 200, c) 400 and d) 2=.
- <6> FFFS, when $d=200$ and $n=100$:  =a) 2, b) 7, c) 15 and d) 200=.
- <7> FFFS, when $d=200$ and $n=400$:  =a) 2, b) 7, c) 15 and d) 200=.

#+BEGIN_SRC python :exports none
import scipy as sp

def numOfParameter(d,c):
    return c*d*(d+3)/2+c-1

n = 100
d = 1
c = 3

while numOfParameter(d,c) < n:
    d += 1

return d
#+END_SRC

#+RESULTS:
: 7
*** KPCA
#+BEGIN_SRC python :exports none
import scipy as sp
import matplotlib.pyplot as plt
from sklearn.decomposition import KernelPCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_circles

n = 100
# Generate data
# mu1 = sp.asarray([0,1])
# mu2 = sp.asarray([0,0])
# X = sp.concatenate((0.1*sp.random.randn(n,2) + mu1, 0.1*sp.random.randn(n,2)+mu2),axis=0)
# Y = sp.concatenate((sp.zeros((n,1))+1,sp.zeros((n,1))+2),axis=0)

X, y = make_circles(n_samples=n, factor=.3, noise=.05)

X[:,0] *= 1000
plt.scatter(X[:,0],X[:,1],c=y,s=30,alpha=0.5)


D = sp.concatenate((X,y[:,sp.newaxis]),axis=1)
sp.savetxt("figures/question_kpca_data.csv",D,delimiter=',')

# Do KPCA without scaling
kpca = KernelPCA(kernel='rbf',gamma=1.0/2,n_jobs=-1)
Xp=kpca.fit_transform(X)[:,:2]
plt.figure()
plt.scatter(Xp[:,0],Xp[:,1],c=y,s=30,alpha=0.5)

D = sp.concatenate((Xp,y[:,sp.newaxis]),axis=1)
sp.savetxt("figures/question_kpca_proj.csv",D,delimiter=',')

# Do KPCA with scaling
sc = StandardScaler()
Xs = sc.fit_transform(X)
plt.figure()
plt.scatter(Xs[:,0],Xs[:,1],c=y,s=30,alpha=0.5)

D = sp.concatenate((Xs,y[:,sp.newaxis]),axis=1)
sp.savetxt("figures/question_kpca_datas.csv",D,delimiter=',')

kpca = KernelPCA(kernel='rbf',gamma=1.0/2,n_jobs=-1)
Xp=kpca.fit_transform(Xs)[:,:2]
plt.figure()
plt.scatter(Xp[:,0],Xp[:,1],c=y,s=30,alpha=0.5)

D = sp.concatenate((Xp,y[:,sp.newaxis]),axis=1)
sp.savetxt("figures/question_kpca_projs.csv",D,delimiter=',')

plt.show()
#+END_SRC

#+RESULTS:
: None

- Given the following data set, using the Gaussian kernel, how do you expect KPCA will behave: 
  
  =a) as usual, b) poorly=
 
- <3-> Gaussian kernel: 
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    k(\mathbf{x}_i,\mathbf{x}_j) = \exp\left[-\frac{\|\mathbf{x}_i-\mathbf{x}_j\|^2}{2\sigma^2}\right]  = \exp\left[-\frac{\sum_{l=1}^d (\mathbf{x}_{il}-\mathbf{x}_{jl})^2}{2\sigma^2}\right]
  \end{eqnarray*}
  #+END_EXPORT
- <4-> _Solution_: scale the features (/e.g./, zero mean and unit variance)

 
#+BEGIN_EXPORT latex
  \begin{center}
    \begin{tabular}{cccc}
      \begin{tikzpicture}
        \begin{axis}[width=0.24\textwidth,height=0.24\textwidth,grid,opacity=0.75]
          \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma,meta index=2,x index=0,y index=1] {figures/question_kpca_data.csv};
        \end{axis}
      \end{tikzpicture}&
          \visible<2->{\begin{tikzpicture}
        \begin{axis}[width=0.24\textwidth,height=0.24\textwidth,grid,opacity=0.75]
          \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma,meta index=2,x index=0,y index=1] {figures/question_kpca_proj.csv};
        \end{axis}
      \end{tikzpicture}}
      &
      \visible<5->{\begin{tikzpicture}
        \begin{axis}[width=0.24\textwidth,height=0.24\textwidth,grid,opacity=0.75]
          \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma,meta index=2,x index=0,y index=1] {figures/question_kpca_datas.csv};
        \end{axis}
      \end{tikzpicture}}&
      \visible<6>{\begin{tikzpicture}
        \begin{axis}[width=0.24\textwidth,height=0.24\textwidth,grid,opacity=0.75]
          \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma,meta index=2,x index=0,y index=1] {figures/question_kpca_projs.csv};
        \end{axis}
      \end{tikzpicture}}
    \end{tabular}

  \end{center}
#+END_EXPORT

* Spatial feature extaction                                          :export:
*** Why spatial feature extraction?
#+BEGIN_EXPORT latex
\centerline{\begin{tabular}{ccc}
\includegraphics[width=0.2\linewidth]{figures/rgb_house_shuffle} & \includegraphics[width=0.2\linewidth]{figures/rgb_house_sort} & \includegraphics[width=0.2\linewidth]{figures/rgb_house_original}\\
\includegraphics[width=0.2\linewidth]{figures/rgb_house_hist} & \includegraphics[width=0.2\linewidth]{figures/rgb_house_hist} & \includegraphics[width=0.2\linewidth]{figures/rgb_house_hist}
\end{tabular}}
#+END_EXPORT
*** More on this topics
#+BEGIN_CENTER
*Image analysis of hyperspectral data using mathematical morphology*
#+END_CENTER

_Tutorial WHISPERS 2014_: Lesson, Labwork and full matlab implementation !

#+BEGIN_VERSE
Dalla Mura, Mauro, & Fauvel, Mathieu. (2014, June). 
Image analysis of hyperspectral data using mathematical morphology. 
Zenodo. http://doi.org/10.5281/zenodo.437195
#+END_VERSE

** Spatial filters
*** Spatial neighborhood
- The neighborhood of a given pixel is the set of pixels that are connected to it.
- For a flat (grayscale) image :
  #+BEGIN_EXPORT latex
  \centerline{\newcounter{a}\newcounter{b}\begin{tikzpicture}
      \foreach \x in {1,...,3}
      \foreach \y in {1,...,3}
      { 
        \draw (\x,\y)+(-.5,-.5) rectangle ++(.5,.5);
        \pgfmathsetcounter{a}{\x-2}
        \pgfmathsetcounter{b}{2-\y}
        \draw (\x,\y) node{$\mathbf{x}_{\thea,\theb}$};
      }
      \draw[red,thick] (1.5,0.5) rectangle +(1,3);
      \draw[red,thick] (0.5,1.5) rectangle +(3,1);
      \draw (2,0) node{4-connected};
    \end{tikzpicture}\hspace{1cm}\begin{tikzpicture}
      \foreach \x in {1,...,3}
      \foreach \y in {1,...,3}
      { 
        \draw (\x,\y)+(-.5,-.5) rectangle ++(.5,.5);
        \pgfmathsetcounter{a}{\x-2}
        \pgfmathsetcounter{b}{2-\y}
        \draw (\x,\y) node{$\mathbf{x}_{\thea,\theb}$};
      }
      \draw[red,thick] (0.5,0.5) rectangle(3.5,3.5);
      \draw (2,0) node{8-connected};
    \end{tikzpicture}}
  #+END_EXPORT
- Wide range of processing are  based on pixel neighborhood
  + De noising,
  + Texture analysis,
  + Edges detection,
  + Pattern recognition,
  + ...
*** Template filters
_Steps_:
1. Define the template  $G$: 4/8-connected and size
2. Define  the processing  $f$ on the  neighborhood. If  $f$ is linear $\leftrightarrow$ convolution.
3. Scan all the pixels:
#+BEGIN_EXPORT latex
$$\mathbf{x}_{ij}^f = f(\mathbf{x}_1,\ldots,\mathbf{x}_N),\ \mathbf{x}_n\in G(i,j)$$
#+END_EXPORT

#+BEGIN_CENTER
Max Filter
#+END_CENTER
#+BEGIN_EXPORT latex
\centerline{\resizebox{0.5\textwidth}{!}{\begin{tikzpicture}
        \foreach \x in {1,...,6}
        \foreach \y in {1,...,6}
        {
          \draw (\x,\y)+(-.5,-.5) rectangle ++(.5,.5);
          \draw (\x,\y) node{\pgfmathparse{int(10*(exp(-\x/10) + exp(\y/10)))}\pgfmathresult};
        }
        \draw[help lines,red,very thick](3.5,3.5) rectangle +(1,1);
        \draw[help lines,red,very thick](2.5,2.5) rectangle +(3,3);
        \foreach \x in {1,...,6}
        \foreach \y in {1,...,6}
        {
          \draw (\x+7,\y)+(-.5,-.5) rectangle ++(.5,.5);
        }
        \draw[help lines,red,very thick](3.5+7,3.5) rectangle +(1,1);
        \draw[red,very thick](11,4) node{23};
        \draw[->,red,thick]  (4.25,4) -- (10.75,4);
      \end{tikzpicture}}}
#+END_EXPORT
*** Some filters
- G = $\begin{bmatrix} 1 & 1  & 1 \\1 & 1  & 1 \\1 & 1  & 1 \end{bmatrix}$, for a $3\times 3$ neighborhood.
- Mean filter
  $$\mathbf{x}^{m}(x,y) = \frac{1}{9}\sum_{i,j=-1}^1\mathbf{x}(x+i,y+j)$$
- Variance filter:
  $$\mathbf{x}^{v}(x,y) = \frac{1}{9}\sum_{i,j=-1}^1\big(\mathbf{x}(x+i,y+j)-\mathbf{x}^{m}(x,y)\big)^2$$
- Range filter:
  $$\mathbf{x}^{r}(x,y) = \max_{i,j\in G}[\mathbf{x}(x+i,y+j)] - \min_{i,j\in G}[\mathbf{x}(x+i,y+j)]$$
- Median filter:
  $$\mathbf{x}^{m}(x,y) = \text{median}_{i,j\in G}[\mathbf{x}(x+i,y+j)]$$
*** Template filters in action 1/3
#+BEGIN_CENTER
*For multidimensional images: Use spectral feature extraction to get flat images!* See [[#spatial:mvi]]
#+END_CENTER

#+BEGIN_SRC sh :tangle ../Codes/template_filter.sh :exports code
# Compute the different filters with a template of size 3x3 and 11x11
for i in 3 11
do
    # Mean filter
    otbcli_BandMathX -il ../Data/pca_university.tif -out ../Data/pca_mean_${i}_${i}_university.tif \
		     -exp "mean(im1b1N${i}x${i}); mean(im1b2N${i}x${i}); mean(im1b3N${i}x${i})"

    # Var filter
    otbcli_BandMathX -il ../Data/pca_university.tif -out ../Data/pca_std_${i}_${i}_university.tif \
		     -exp "var(im1b1N${i}x${i}); var(im1b2N${i}x${i}); var(im1b3N${i}x${i})"

    # Range filter
    otbcli_BandMathX -il ../Data/pca_university.tif -out ../Data/pca_range_${i}_${i}_university.tif \
		     -exp "vmax(im1b1N${i}x${i})-vmin(im1b1N${i}x${i}); vmax(im1b2N${i}x${i})-vmin(im1b2N${i}x${i});\
                     vmax(im1b3N${i}x${i})-vmin(im1b3N${i}x${i})"

    # Median filter
    otbcli_BandMathX -il ../Data/pca_university.tif -out ../Data/pca_median_${i}_${i}_university.tif \
		     -exp "median(im1b1N${i}x${i}); median(im1b2N${i}x${i}); median(im1b3N${i}x${i})"
done
#+END_SRC

*** Template filters in action 2/3
#+BEGIN_CENTER
#+ATTR_LATEX: :width 0.22\linewidth :center :options [trim=2.944cm 8.832cm 2.944cm 10.304cm, clip=true]
[[file:./figures/university_pc1.png]]
#+END_CENTER

**** Mean                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 6cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1_mean_3_3.png]]
**** STD                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 6cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1_std_3_3.png]]
**** Range                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 6cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1_range_3_3.png]]
**** Median                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 6cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1_median_3_3.png]]
*** Template filters in action 3/3
#+BEGIN_CENTER
#+ATTR_LATEX: :width 0.22\linewidth :center :options [trim=2.944cm 8.832cm 2.944cm 10.304cm, clip=true]
[[file:./figures/university_pc1.png]]
#+END_CENTER

**** Mean                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 6cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1_mean_11_11.png]]
**** STD                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 6cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1_std_11_11.png]]
**** Range                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 6cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1_range_11_11.png]]
**** Median                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 6cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1_median_11_11.png]]
** Mathematical morphology
*** Dilation and erosion
- Mathematical morphology: non-linear image processing.
- A lot of applications in geoscience and remote sensing, see cite:soille:pesaresi
- _Erosion_: template filter with a $\min$ operation in $G$ (called /structuring element/)
- _Dilation_: template filter with a $\max$ operation in $G$

**** erosion                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :float t :center :width 0.85\linewidth :height 0.85\linewidth
[[file:./figures/ero.pdf]]
**** original                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :float t :center :width 0.85\linewidth :height 0.85\linewidth
[[file:./figures/orig.pdf]]
**** dilation                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :float t :center :width 0.85\linewidth :height 0.85\linewidth
[[file:./figures/dil.pdf]]
*** Effets of structuring elements
**** diamond                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :float t :center :width 0.85\linewidth :height 0.85\linewidth
[[file:./figures/dil_disk.png]]

#+ATTR_LATEX: :align |c|c|c|c|c|
|---+---+---+---+---|
| 0 | 0 | 1 | 0 | 0 |
|---+---+---+---+---|
| 0 | 1 | 1 | 1 | 0 |
|---+---+---+---+---|
| 1 | 1 | 1 | 1 | 1 |
|---+---+---+---+---|
| 0 | 1 | 1 | 1 | 0 |
|---+---+---+---+---|
| 0 | 0 | 1 | 0 | 0 |
|---+---+---+---+---|
**** square                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :float t :center :width 0.85\linewidth :height 0.85\linewidth
[[file:./figures/dil_square.png]]

#+ATTR_LATEX: :align |c|c|c|c|c|
|---+---+---+---+---|
| 0 | 0 | 0 | 0 | 0 |
|---+---+---+---+---|
| 0 | 1 | 1 | 1 | 0 |
|---+---+---+---+---|
| 0 | 1 | 1 | 1 | 0 |
|---+---+---+---+---|
| 0 | 1 | 1 | 1 | 0 |
|---+---+---+---+---|
| 0 | 0 | 0 | 0 | 0 |
|---+---+---+---+---|

**** line                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :float t :center :width 0.85\linewidth :height 0.85\linewidth
[[file:./figures/dil_line.png]]

#+ATTR_LATEX: :align |c|c|c|c|c|
|---+---+---+---+---|
| 0 | 0 | 0 | 0 | 1 |
|---+---+---+---+---|
| 0 | 0 | 0 | 1 | 0 |
|---+---+---+---+---|
| 0 | 0 | 1 | 0 | 0 |
|---+---+---+---+---|
| 0 | 1 | 0 | 0 | 0 |
|---+---+---+---+---|
| 1 | 0 | 0 | 0 | 0 |
|---+---+---+---+---|

*** Opening and closing
- _Opening_:
  + /Erosion/ followed by a /dilation/
  + Remove bright objects that are smaller than the SE
- <2> _Opening by reconstruction_:
  + /Erosion/ followed by a /reconstruction/
  + /Completely/ removes bright objects that are smaller than the SE, otherwise preserve it
- _Closing_:
  + /Dilation/ followed by an /erosion/
  + Remove dark objects that are smaller than the SE
- <2> _Closing by reconstruction_:
  + /Dilation/ followed by an /erosion/
  + /Completely/ removes dark objects that are smaller than the SE, otherwise preserve it

**** Closing                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.2
:END:
#+ATTR_LATEX: :float t :center :width 1\linewidth :height 1\linewidth
[[file:./figures/close.pdf]]
**** Closing by reconstruction                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.2
:END:
#+LaTeX: \only<2>{
#+ATTR_LATEX: :float t :center :width 1\linewidth :height 1\linewidth
[[file:./figures/geo_close.pdf]]
#+LaTeX: }
**** Opening                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.2
:END:
#+ATTR_LATEX: :float t :center :width 1\linewidth :height 1\linewidth
[[file:./figures/open.pdf]]
**** Closing by reconstruction                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.2
:END:
#+LaTeX: \only<2>{
#+ATTR_LATEX: :float t :center :width 1\linewidth :height 1\linewidth
[[file:./figures/geo_open.pdf]]
#+LaTeX: }
*** Opening and closing profile
- For a  given $B$,  $\gamma_{B}^r$  (resp. $\phi_{B}^r$)   indicates which clear (dark) objects fit $B$.
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}[]
      \node at (0,0) {\includegraphics[width=0.2\textwidth,height=0.215\textheight]{figures/orig}};
      \node at (5,0) {\includegraphics[width=0.2\textwidth,height=0.215\textheight]{figures/geo_open}};
      \node at (10,0) {\includegraphics[width=0.2\textwidth,height=0.215\textheight]{figures/geo_close}};
      \draw[->,red,very thick] (-0.9,-0.85) -- node[above] {$\gamma_{B}^r$} (4,-0.85);
      \draw[->,red,very thick] (-0.85,-0.15) -- node[above] {$\phi_{B}^r$} (9,-0.15);
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
- Applying $\gamma_{B_i}$ with a set of $\big\{B_i|B_{i}\subset B_{i+1},i\in[1,\ldots,n]\big\}$ : \textcolor{magenta}{Opening Profile}
- Applying $\phi_{B_i}$ with a set of $\big\{B_i|B_{i}\subset B_{i+1},i\in[1,\ldots,n]\big\}$ : \textcolor{magenta}{Closing Profile}
** Extension to multivalued images
:PROPERTIES:
:CUSTOM_ID: spatial:mvi
:END:

*** Ordering relation
- MM is based on $\inf$ and $\sup$ operators
- No unambiguous $\inf$ / $\sup$ for pixel/vector:
    $$\begin{bmatrix}1\\5\\2\end{bmatrix} \overset{?}{\lessgtr} \begin{bmatrix}0\\6\\1\end{bmatrix}$$
- Marginal ordering $\Rightarrow$ by band filtering
- Reduced ordering $\Rightarrow h:\mathbb{R}^d\to\mathbb{R}$
  $$\mathbf{x}\mapsto h(x)$$
- Use spectral feature extraction /then/ spatial feature extraction.

** Questions
*** Noise filtering
- The image has been corrupted by a /salt and pepper/ noise, which filter should we use to filter it ?

  =a) mean filter, b) opening, c) median filter and d) closing=.
- To remove the small cars on the road, which filter should we use to filter it ?

  =a) mean filter, b) opening, c) median filter and d) closing=.

**** Original                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 7cm 2cm 7cm, clip=true]
[[file:./figures/university_pc1.png]]

**** Noise                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 7cm 2cm 7cm, clip=true]
[[file:./figures/university_noise.png]]

**** Median                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+LaTeX: \only<2->{
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 7cm 2cm 7cm, clip=true]
[[file:./figures/university_noise_filtered.png]]
#+LaTeX: }

**** Opening by reconstruction                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.22
:END:
#+LaTeX: \only<3>{
#+ATTR_LATEX: :width 1\linewidth :center :options [trim=2cm 7cm 2cm 7cm, clip=true]
[[file:./figures/university_open_2.png]]
#+LaTeX: }

* References                                                         :export:
*** Bibliography
  :PROPERTIES:
  :BEAMER_OPT: fragile,allowframebreaks,label=
  :END:      
  \printbibliography
*** 
#+BEGIN_CENTER
\tiny Creative Commons Attribution-ShareAlike 4.0 Unported License
\normalsize

#+ATTR_LATEX: :width 0.1\textwidth
[[file:figures/cc-by-sa.png]]
#+END_CENTER
* Figures                                                          :noexport:
** PCA
#+BEGIN_SRC python :session :results output
# Load data
from sklearn.datasets.samples_generator import make_classification
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import scipy as sp


# Generate samples
X, y = make_classification(n_samples=100, n_classes=1, n_features=2, n_informative=2,n_redundant=0,random_state=1,n_clusters_per_class=1)

# Do PCA
pca = PCA(n_components=2)
pca.fit(X)

print(pca.explained_variance_ratio_)
print(pca.components_)

# Compute the eigenvector
for i in range(2):
      print(X.mean(axis=0)-pca.components_[i])
      print(X.mean(axis=0)+pca.components_[i])
      

# Save data
sp.savetxt("figures/pca_data.csv",X,delimiter=',')
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> >>> >>> >>> >>> ... >>> >>> ... >>> PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)
>>> [ 0.84182344  0.15817656]
[[ 0.9899841   0.14117892]
 [ 0.14117892 -0.9899841 ]]
>>> ... ... ... ... ... [ 0.080264    0.83834891]
[ 2.06023219  1.12070676]
[ 0.92906917  1.96951193]
[ 1.21142702 -0.01045626]
#+end_example
** KPCA
#+BEGIN_SRC python :session :results output
from sklearn.datasets import make_circles
from sklearn.decomposition import KernelPCA
import scipy as sp

# Data generation
X, y = make_circles(n_samples=400, factor=.3, noise=.05)
# Do KPCA
kpca = KernelPCA(kernel="rbf",gamma=5)
Xp = kpca.fit_transform(X)

# Get eigenvalues
l = kpca.lambdas_
cl = l.cumsum()/l.sum()

for i in range(10):
    print "({0},{1})".format(i+1,l[i])

for i in range(10):
    print "({0},{1})".format(i+1,cl[i])

# Save results
sp.savetxt("figures/kpca_data.csv",sp.concatenate((X,y[:,sp.newaxis]),axis=1),delimiter=',')
sp.savetxt("figures/kpca_datap.csv",sp.concatenate((Xp[:,:2],y[:,sp.newaxis]),axis=1),delimiter=',')
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> >>> ... >>> ... >>> >>> >>> ... >>> >>> >>> ... ... (1,57.4446834929)
(2,40.6516908637)
(3,39.5265970358)
(4,23.2170239146)
(5,22.7561859043)
(6,20.9207054308)
(7,20.5376050443)
(8,15.914586718)
(9,15.4870029584)
(10,11.4444709279)
... ... (1,0.171950045779)
(2,0.293633371022)
(3,0.41194893578)
(4,0.481444806977)
(5,0.54956124474)
(6,0.612183510855)
(7,0.673659036749)
(8,0.721296411495)
(9,0.767653893262)
(10,0.80191080235)
#+end_example
** FDA
#+BEGIN_SRC python :results output
from sklearn.datasets.samples_generator import make_classification
import scipy as  sp
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Generate data
x,y = make_classification(n_samples=200,n_classes=2,n_features=2,n_informative=2,n_redundant=0,n_clusters_per_class=1,random_state=1,class_sep=2)
x-=x.mean(axis=0)
# Save data
D = sp.concatenate((x,y[:,sp.newaxis]),axis=1)
sp.savetxt("figures/lda_data.csv",D,delimiter=',')
# Apply LDA
clf = LinearDiscriminantAnalysis(solver='eigen')
clf.fit(x, y)
print clf.coef_
print clf.intercept_
#+END_SRC

#+RESULTS:
: [[ 1.86816687 -3.04406312]]
: [-0.08378251]

** FFFS

** Noise in figure
#+BEGIN_SRC python 
import rasterTools as rt
import scipy as sp

# Load image
im,Geo,Proj=rt.open_data("../Data/pca_university.tif")
[H,W,B]=im.shape

# Add noise
im = im[:,:,0].reshape(H,W)
M,m = im.max(),im.min()

prob = 0.2
rnd = sp.random.rand(H, W)
noisyImage = im.copy()
noisyImage[rnd<prob]=m
noisyImage[rnd>1-prob]=M

rt.write_data("../Data/university_noise.tif",noisyImage,Geo,Proj)
#+END_SRC



* Todo                                                             :noexport:
- [X] Add questions for stastistical feature extraction
- [X] Add questions for spatial feature extraction
- [ ] Add simulations for stastitical feature extraction
- [ ] Add some references
